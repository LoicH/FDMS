{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# TME 3 - Régularisation L1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of X: (569, 30)\n",
      "shape of Y: (569,)\n",
      "classes in Y:  (array([0, 1]), array([212, 357]))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn import metrics\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "import functools\n",
    "import operator\n",
    "import itertools\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Loading 'breast cancer' dataset, a simple dataset\n",
    "cancer = datasets.load_breast_cancer()\n",
    "X = cancer.data\n",
    "y = cancer.target\n",
    "\n",
    "X, y = shuffle(X, y)\n",
    "\n",
    "# Decrease data to speed up computation\n",
    "# X = X[:50,:5]\n",
    "# y = y[:50]\n",
    "\n",
    "print(\"shape of X:\", X.shape)\n",
    "print(\"shape of Y:\", y.shape)\n",
    "print(\"classes in Y: \", np.unique(y, return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking gradient_loss with random theta:\n",
      "\n",
      "### Iteration 0 ###\n",
      "Theta :                                [ 4262.5  2658.5 -3385.7   850.3  2703.2]\n",
      "Gradient according to approx_fprime(): [  1.5e+07   1.8e+07   9.6e+07   8.2e+08   8.3e+04]\n",
      "Gradient according to gradient_loss(): [  1.5e+07   1.8e+07   9.6e+07   8.2e+08   8.3e+04]\n",
      "Error according to check_grad(): 3845.560374 (0.000467%)\n",
      "\n",
      "### Iteration 1 ###\n",
      "Theta :                                [ 21457.4 -23410.2  25486.9  12351.4 -44250.4]\n",
      "Gradient according to approx_fprime(): [  3.9e+08   4.9e+08   2.6e+09   2.1e+10   2.3e+06]\n",
      "Gradient according to gradient_loss(): [  3.9e+08   4.9e+08   2.6e+09   2.1e+10   2.3e+06]\n",
      "Error according to check_grad(): 4633054.184045 (0.021467%)\n",
      "\n",
      "### Iteration 2 ###\n",
      "Theta :                                [ 13.7  18.1  43.   37.8  18.5]\n",
      "Gradient according to approx_fprime(): [  1.1e+06   1.4e+06   7.4e+06   6.2e+07   6.6e+03]\n",
      "Gradient according to gradient_loss(): [  1.1e+06   1.4e+06   7.4e+06   6.1e+07   6.6e+03]\n",
      "Error according to check_grad(): 43.133412 (0.000070%)\n",
      "\n",
      "### Iteration 3 ###\n",
      "Theta :                                [ -26.7  210.7 -147.1  459.   136.7]\n",
      "Gradient according to approx_fprime(): [  1.1e+07   1.4e+07   7.5e+07   6.3e+08   6.6e+04]\n",
      "Gradient according to gradient_loss(): [  1.1e+07   1.4e+07   7.5e+07   6.3e+08   6.6e+04]\n",
      "Error according to check_grad(): 2189.426017 (0.000347%)\n",
      "\n",
      "### Iteration 4 ###\n",
      "Theta :                                [ 24678.3  27226.2  -2264.2  14146.5 -10275.2]\n",
      "Gradient according to approx_fprime(): [  3.8e+08   4.8e+08   2.5e+09   2.1e+10   2.2e+06]\n",
      "Gradient according to gradient_loss(): [  3.8e+08   4.8e+08   2.5e+09   2.1e+10   2.2e+06]\n",
      "Error according to check_grad(): 5513794.531508 (0.026101%)\n",
      "\n",
      "### Iteration 5 ###\n",
      "Theta :                                [ -315957.5  -428384.3  3707469.8 -4318907.6 -1232065.7]\n",
      "Gradient according to approx_fprime(): [ -9.9e+10  -1.2e+11  -6.5e+11  -5.5e+12  -5.7e+08]\n",
      "Gradient according to gradient_loss(): [ -9.9e+10  -1.2e+11  -6.5e+11  -5.5e+12  -5.7e+08]\n",
      "Error according to check_grad(): 276757525166.652771 (5.028890%)\n",
      "\n",
      "### Iteration 6 ###\n",
      "Theta :                                [ 205677.   252402.3 -465353.2 -323884.  -120211. ]\n",
      "Gradient according to approx_fprime(): [ -9.5e+09  -1.2e+10  -6.2e+10  -5.2e+11  -5.5e+07]\n",
      "Gradient according to gradient_loss(): [ -9.5e+09  -1.2e+10  -6.2e+10  -5.2e+11  -5.5e+07]\n",
      "Error according to check_grad(): 3540024129.269706 (0.676162%)\n",
      "\n",
      "### Iteration 7 ###\n",
      "Theta :                                [ -119.3  2845.4 -2648.9 -4114.  -3401.8]\n",
      "Gradient according to approx_fprime(): [ -1.1e+08  -1.4e+08  -7.3e+08  -6.1e+09  -6.5e+05]\n",
      "Gradient according to gradient_loss(): [ -1.1e+08  -1.4e+08  -7.3e+08  -6.1e+09  -6.5e+05]\n",
      "Error according to check_grad(): 154477.272809 (0.002500%)\n",
      "\n",
      "### Iteration 8 ###\n",
      "Theta :                                [  88.8 -227.8  455.3 -322.  -366.3]\n",
      "Gradient according to approx_fprime(): [ -6.8e+06  -8.4e+06  -4.5e+07  -3.8e+08  -3.9e+04]\n",
      "Gradient according to gradient_loss(): [ -6.8e+06  -8.4e+06  -4.5e+07  -3.8e+08  -3.9e+04]\n",
      "Error according to check_grad(): 919.298969 (0.000240%)\n",
      "\n",
      "### Iteration 9 ###\n",
      "Theta :                                [-16.3  25.3 -25.7  13.9 -31. ]\n",
      "Gradient according to approx_fprime(): [  2.8e+05   3.4e+05   1.8e+06   1.6e+07   1.6e+03]\n",
      "Gradient according to gradient_loss(): [  2.8e+05   3.4e+05   1.8e+06   1.6e+07   1.6e+03]\n",
      "Error according to check_grad(): 1.920630 (0.000012%)\n"
     ]
    }
   ],
   "source": [
    "from scipy.optimize import approx_fprime, check_grad, minimize\n",
    "\n",
    "\n",
    "def decision(theta, X):\n",
    "    \"\"\" The decision function that predicts a target\n",
    "    :param theta: The parameter for the decision\n",
    "    :param X: The data.\n",
    "    \n",
    "    theta.shape = (n,) and X.shape = (l, n)\n",
    "    :return: a vector of predictions of shape (l,)\"\"\"\n",
    "    return np.sign(X.dot(theta)) / 2 + 0.5\n",
    "\n",
    "def loss(theta, X, y, pen):\n",
    "    \"\"\" The loss function\n",
    "    :param theta: The vector doing the decision\n",
    "    :param X: The data\n",
    "    :param y: The target values\n",
    "    :param pen: The penalization parameter (The higher 'pen', the lower |theta|)\n",
    "    :return: The loss for the given parameters.\n",
    "    \"\"\"\n",
    "    mse = ((y - X.dot(theta))**2).mean()\n",
    "    return mse + pen * sum([np.abs(t) for t in theta])\n",
    "\n",
    "def gradient_loss(theta, X, y, pen):\n",
    "    \"\"\" The gradient of the loss function\n",
    "    :param theta: The vector doing the decision\n",
    "    :param X: The data\n",
    "    :param y: The target values\n",
    "    :param pen: The penalization parameter\n",
    "    :return: The gradient of the for the given parameters.\n",
    "    \"\"\"\n",
    "    gradient = 2 * X.T.dot(X.dot(theta) - y) / X.shape[0] + pen * np.sign(theta)\n",
    "    return gradient\n",
    "\n",
    "### Testing if the gradient is correct ### \n",
    "print(\"Checking gradient_loss with random theta:\")\n",
    "np.set_printoptions(precision=1, threshold=5)\n",
    "for i in range(10):\n",
    "    print(\"\\n### Iteration %d ###\" % i)\n",
    "    theta = np.random.random(X[0].shape) - 0.5\n",
    "    theta *= 10**np.random.randint(1, 8)\n",
    "    pen = np.random.random() / 10\n",
    "    func = lambda t:loss(t, X, y, pen)\n",
    "    grad = lambda t:gradient_loss(t, X, y, pen)\n",
    "    print(\"Theta :                               \", theta)\n",
    "    print(\"Gradient according to approx_fprime():\", approx_fprime(theta, func, epsilon=1))\n",
    "    print(\"Gradient according to gradient_loss():\", grad(theta))\n",
    "    err = check_grad(func, grad, theta)\n",
    "    print(\"Error according to check_grad(): %f (%f%%)\" % (err, err / np.linalg.norm(grad(theta)) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "theta with gradient_l1(): [-0.08 -0.05 -0.04 ..., -0.06 -0.45 -0.12]\n",
      "theta with minimize()   : [  4.32e-01  -2.04e-03  -2.50e-02 ...,  -5.31e-01  -6.61e-01  -6.48e+00]\n",
      "Errors: 210/569\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nKVdTAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff961fb7be0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def gradient_l1(theta, X, y, max_iter=100, step=1e-1, pen=0.1, verbose=0, plot=False, force=False):\n",
    "    \"\"\" Performs gradient clipping to find the best theta\n",
    "    :param theta: The initial scalar for the search\n",
    "    :param X: The data\n",
    "    :param y: The target values\n",
    "    :param max_iter: The number of iterations\n",
    "    :param step: The scalar the will be multiplied to the gradient vector to increment theta\n",
    "    :param pen: The regularization parameter\n",
    "    :param verbose: The level of displayed information\n",
    "    :param plot: Whether to plot the evolution of the loss\n",
    "    :param force: Whether to deactivate the warning if too much output\n",
    "    \n",
    "    :return: The theta at the end of the gradient descent\"\"\"\n",
    "    \n",
    "    (l, n) = X.shape\n",
    "    losses = []\n",
    "    \n",
    "    if not force and verbose > 1 and max_iter > 50:\n",
    "        print(\"Warning, there will be a lot of output, it can freeze the kernel.\")\n",
    "        print(\"If you really want to proceed, relaunch with 'force=True'\")\n",
    "        return \n",
    "    \n",
    "    for it in range(max_iter):\n",
    "        if verbose >= 1:\n",
    "            print(\"###########  Step\", it, \": #################\")\n",
    "        for i in range(l):\n",
    "            # Sample a random point \n",
    "            idx = np.random.randint(0, l)\n",
    "            if verbose >= 2:\n",
    "                print(\"========= i = %d, random idx = %d ===========\" % (i, idx))\n",
    "            # Compute the gradient on this single random point:\n",
    "            local_loss = lambda t: loss(t, np.array([X[idx]]), np.array([y[idx]]), pen)\n",
    "#             grad = approx_fprime(theta, local_loss, epsilon=step)\n",
    "            \n",
    "            grad = gradient_loss(theta, np.array([X[idx]]), np.array([y[idx]]), pen)\n",
    "            \n",
    "            #debug\n",
    "            if verbose >=2:\n",
    "                local_grad = lambda t: gradient_loss(theta, np.array([X[idx]]), np.array([y[idx]]), pen)\n",
    "                print(\"debug: check_grad:\", check_grad(local_loss, local_grad, theta))\n",
    "                \n",
    "            if verbose >= 2:\n",
    "                print(\"Gradient:     \",grad)\n",
    "                print(\"Theta before: \", theta)\n",
    "                \n",
    "#             step = minimize(lambda s: local_loss(theta - s*grad), step).x\n",
    "\n",
    "            theta_prime = theta - step * grad\n",
    "            theta_prime[theta * theta_prime < 0] = 0\n",
    "            theta = theta_prime\n",
    "            \n",
    "            if verbose >= 2:\n",
    "                print(\"Step: \", step)\n",
    "                print(\"Theta after:  \", theta)\n",
    "                \n",
    "        general_loss = loss(theta, X, y, pen)\n",
    "        losses.append(general_loss)\n",
    "        if verbose >= 1:\n",
    "            print(\"Loss :     %f\" % (general_loss))\n",
    "            errors = np.count_nonzero(decision(theta, X) - y)\n",
    "            print(\"Accuracy : %d/%d right guesses\" % (l - errors, l))\n",
    "    \n",
    "    if plot:\n",
    "        plt.plot(losses)\n",
    "        plt.title(\"Evolution of loss\")\n",
    "        plt.xlabel(\"Iterations\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        \n",
    "    return theta\n",
    "\n",
    "# Optimization parameters\n",
    "theta_init = np.random.random(X[0].shape) - 0.5\n",
    "max_iter = 200\n",
    "step     = 1e-7\n",
    "pen      = 0\n",
    "\n",
    "# Find theta \n",
    "np.set_printoptions(precision=2, threshold=5)\n",
    "theta = gradient_l1(theta_init, X, y, max_iter=max_iter, pen=pen, step=step, verbose=0, plot=True)\n",
    "theta_minimize = minimize(lambda t:loss(t, X, y, pen), theta_init).x\n",
    "print(\"theta with gradient_l1():\", theta)\n",
    "print(\"theta with minimize()   :\", theta_minimize)\n",
    "\n",
    "errs = np.count_nonzero(decision(theta, X) - y)\n",
    "print(\"Errors: %d/%d\" % (errs, len(X)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "class LinearClassifier(BaseEstimator):\n",
    "    \"\"\" Custom linear classifier \"\"\"\n",
    "    \n",
    "    def __init__(self, max_iter=1000, step=1e-2, pen=1e-2, verbose=0):\n",
    "        self.theta= 0\n",
    "        self.max_iter = max_iter\n",
    "        self.step = step\n",
    "        self.pen = pen\n",
    "        self.verbose = verbose\n",
    "    \n",
    "    def fit(self, X, y):        \n",
    "        self.theta = gradient_l1(np.ones_like(X[0]), X, y, \n",
    "                                 max_iter=self.max_iter, \n",
    "                                 step=self.step, \n",
    "                                 pen=self.pen,\n",
    "                                 verbose=self.verbose)\n",
    "        \n",
    "    def predict(self, X):\n",
    "        return decision(self.theta, X)\n",
    "\n",
    "    def get_theta(self):\n",
    "        return self.theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the classifier:\n",
      "--------------------\n",
      "y_pred:  [ 0.  1.  1. ...,  0.  1.  1.]\n",
      "y_reel:  [1 1 1 ..., 1 0 1]\n",
      "Score:  0.587719298246\n",
      "Errors on following labels:  (array([0, 1]), array([22, 25]))\n",
      "Errors on the training set: 237/455\n",
      "--------------------\n",
      "y_pred:  [ 1.  1.  0. ...,  0.  0.  1.]\n",
      "y_reel:  [1 1 1 ..., 1 1 1]\n",
      "Score:  0.517543859649\n",
      "Errors on following labels:  (array([0, 1]), array([21, 34]))\n",
      "Errors on the training set: 219/455\n",
      "--------------------\n",
      "y_pred:  [ 0.  1.  1. ...,  0.  1.  1.]\n",
      "y_reel:  [1 0 1 ..., 1 1 1]\n",
      "Score:  0.482456140351\n",
      "Errors on following labels:  (array([0, 1]), array([44, 15]))\n",
      "Errors on the training set: 219/455\n",
      "--------------------\n",
      "y_pred:  [ 1.  0.  0. ...,  1.  1.  1.]\n",
      "y_reel:  [0 1 1 ..., 0 0 0]\n",
      "Score:  0.447368421053\n",
      "Errors on following labels:  (array([0, 1]), array([36, 27]))\n",
      "Errors on the training set: 242/455\n",
      "--------------------\n",
      "y_pred:  [ 1.  1.  1. ...,  1.  1.  1.]\n",
      "y_reel:  [0 0 1 ..., 1 0 1]\n",
      "Score:  0.587719298246\n",
      "Errors on following labels:  (array([0, 1]), array([44,  3]))\n",
      "Errors on the training set: 181/455\n",
      "--------------------\n",
      "y_pred:  [ 1.  0.  0. ...,  0.  1.  1.]\n",
      "y_reel:  [0 1 1 ..., 1 1 0]\n",
      "Score:  0.491228070175\n",
      "Errors on following labels:  (array([0, 1]), array([22, 36]))\n",
      "Errors on the training set: 232/455\n",
      "--------------------\n",
      "y_pred:  [ 1.  0.  1. ...,  1.  0.  1.]\n",
      "y_reel:  [1 1 0 ..., 1 1 0]\n",
      "Score:  0.491228070175\n",
      "Errors on following labels:  (array([0, 1]), array([35, 23]))\n",
      "Errors on the training set: 216/455\n",
      "--------------------\n",
      "y_pred:  [ 1.  1.  0. ...,  1.  0.  1.]\n",
      "y_reel:  [1 0 1 ..., 1 1 0]\n",
      "Score:  0.456140350877\n",
      "Errors on following labels:  (array([0, 1]), array([18, 44]))\n",
      "Errors on the training set: 233/455\n",
      "--------------------\n",
      "y_pred:  [ 1.  1.  0. ...,  0.  0.  1.]\n",
      "y_reel:  [0 1 0 ..., 0 1 1]\n",
      "Score:  0.508771929825\n",
      "Errors on following labels:  (array([0, 1]), array([21, 35]))\n",
      "Errors on the training set: 210/455\n",
      "--------------------\n",
      "y_pred:  [ 1.  0.  1. ...,  1.  0.  1.]\n",
      "y_reel:  [1 1 0 ..., 1 1 0]\n",
      "Score:  0.464912280702\n",
      "Errors on following labels:  (array([0, 1]), array([26, 35]))\n",
      "Errors on the training set: 224/455\n",
      "####################\n",
      "\n",
      "Evaluating with cross validation\n",
      "--------------------\n",
      "y_pred:  [ 1.  1.  1. ...,  1.  1.  0.]\n",
      "y_reel:  [1 0 1 ..., 0 0 1]\n",
      "Score:  0.508771929825\n",
      "Errors on following labels:  (array([0, 1]), array([14, 14]))\n",
      "Errors on the training set: 256/512\n",
      "--------------------\n",
      "y_pred:  [ 1.  1.  1. ...,  1.  1.  1.]\n",
      "y_reel:  [1 0 1 ..., 1 0 0]\n",
      "Score:  0.614035087719\n",
      "Errors on following labels:  (array([0, 1]), array([13,  9]))\n",
      "Errors on the training set: 249/512\n",
      "--------------------\n",
      "y_pred:  [ 1.  1.  1. ...,  1.  1.  1.]\n",
      "y_reel:  [1 0 1 ..., 1 0 0]\n",
      "Score:  0.561403508772\n",
      "Errors on following labels:  (array([0, 1]), array([18,  7]))\n",
      "Errors on the training set: 252/512\n",
      "--------------------\n",
      "y_pred:  [ 0.  0.  0. ...,  1.  0.  0.]\n",
      "y_reel:  [1 1 1 ..., 1 1 1]\n",
      "Score:  0.526315789474\n",
      "Errors on following labels:  (array([0, 1]), array([ 7, 20]))\n",
      "Errors on the training set: 226/512\n",
      "--------------------\n",
      "y_pred:  [ 0.  1.  0. ...,  1.  1.  1.]\n",
      "y_reel:  [1 1 1 ..., 0 1 0]\n",
      "Score:  0.438596491228\n",
      "Errors on following labels:  (array([0, 1]), array([24,  8]))\n",
      "Errors on the training set: 247/512\n",
      "--------------------\n",
      "y_pred:  [ 0.  1.  1. ...,  0.  0.  1.]\n",
      "y_reel:  [0 1 1 ..., 1 1 1]\n",
      "Score:  0.368421052632\n",
      "Errors on following labels:  (array([0, 1]), array([15, 21]))\n",
      "Errors on the training set: 242/512\n",
      "--------------------\n",
      "y_pred:  [ 0.  0.  1. ...,  1.  1.  1.]\n",
      "y_reel:  [1 1 0 ..., 1 1 1]\n",
      "Score:  0.456140350877\n",
      "Errors on following labels:  (array([0, 1]), array([21, 10]))\n",
      "Errors on the training set: 267/512\n",
      "--------------------\n",
      "y_pred:  [ 1.  1.  1. ...,  0.  0.  1.]\n",
      "y_reel:  [1 1 0 ..., 1 1 1]\n",
      "Score:  0.578947368421\n",
      "Errors on following labels:  (array([0, 1]), array([16,  8]))\n",
      "Errors on the training set: 256/512\n",
      "--------------------\n",
      "y_pred:  [ 1.  1.  1. ...,  1.  1.  0.]\n",
      "y_reel:  [1 1 1 ..., 0 1 1]\n",
      "Score:  0.473684210526\n",
      "Errors on following labels:  (array([0, 1]), array([13, 17]))\n",
      "Errors on the training set: 241/512\n",
      "--------------------\n",
      "y_pred:  [ 1.  1.  1. ...,  0.  0.  1.]\n",
      "y_reel:  [1 0 0 ..., 0 1 1]\n",
      "Score:  0.535714285714\n",
      "Errors on following labels:  (array([0, 1]), array([21,  5]))\n",
      "Errors on the training set: 271/513\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "\n",
    "\n",
    "\n",
    "print(\"Evaluating the classifier:\")\n",
    "for it in range(10):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "    clf = LinearClassifier(max_iter=max_iter, step=step, pen=pen, verbose=0)\n",
    "    clf.fit(X_train, y_train)\n",
    "    pred = clf.predict(X_test)\n",
    "    print(20 * '-')\n",
    "    print(\"y_pred: \", pred)\n",
    "    print(\"y_reel: \", y_test)\n",
    "    print(\"Score: \", accuracy_score(pred, y_test))\n",
    "    print(\"Errors on following labels: \", np.unique(y_test[pred != y_test], return_counts=True))\n",
    "    errs = np.count_nonzero(decision(clf.get_theta(), X_train) - y_train)\n",
    "    print(\"Errors on the training set: %d/%d\" % (errs, len(X_train)))\n",
    "\n",
    "\n",
    "print(20*'#' + '\\n')\n",
    "print(\"Evaluating with cross validation\")\n",
    "kf = KFold(n_splits=10)\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    clf = LinearClassifier(max_iter=max_iter, step=step, pen=pen, verbose=0)\n",
    "    clf.fit(X_train, y_train)\n",
    "    pred = clf.predict(X_test)\n",
    "    print(20 * '-')\n",
    "    print(\"y_pred: \", pred)\n",
    "    print(\"y_reel: \", y_test)\n",
    "    print(\"Score: \", accuracy_score(pred, y_test))\n",
    "    print(\"Errors on following labels: \", np.unique(y_test[pred != y_test], return_counts=True))\n",
    "    errs = np.count_nonzero(decision(clf.get_theta(), X_train) - y_train)\n",
    "    print(\"Errors on the training set: %d/%d\" % (errs, len(X_train)))\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Q4) Performance en fonction de la parcimonie du modèle\n",
    "\n",
    "**Performance** : pourcentage de bonne classification sur un ensemble de test\n",
    "\n",
    "**Parcimonie** : Le nombre d'éléments différents de 0, divisé par le nombre total d'éléments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD8CAYAAACb4nSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nMvL/AcuVcmd9eBgNAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc796266828>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def sparsity(vect):\n",
    "    return np.count_nonzero(vect)/len(vect)\n",
    "\n",
    "scores = []\n",
    "spars = []\n",
    "kf = KFold(n_splits=5)\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    clf = LinearClassifier(max_iter=20, step=step, pen=1e-1, verbose=2)\n",
    "    clf.fit(X_train, y_train)\n",
    "    pred = clf.predict(X_test)\n",
    "    scores.append(accuracy_score(pred, y_test))\n",
    "    spars.append(sparsity(clf.get_theta()))\n",
    "    \n",
    "plt.scatter(spars, scores);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Apparemment, aucun des vecteurs n'a de composante nulle..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
