{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/melkigabriel/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_mldata as ldata\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import datasets\n",
    "from scipy.linalg import solve\n",
    "from scipy.sparse import linalg, eye\n",
    "from scipy import stats\n",
    "from scipy.spatial import distance\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/melkigabriel/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:8: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "movie = pd.read_csv('/Users/melkigabriel/Downloads/ml-100k/u5.base',\n",
    "                    sep='\\t',\n",
    "                    names =['user_id','item_id','rating','timestamp'])\n",
    "movie.head()\n",
    "\n",
    "ratings= pd.read_csv('/Users/melkigabriel/Downloads/ml-1m/ratings.dat',\n",
    "                    sep='::',\n",
    "                    names =['user_id','item_id','rating','timestamp'])\n",
    "\n",
    "movie = movie\n",
    "t = pd.crosstab(movie['user_id'],  movie['item_id']) #Matrice de filtre, =1 s'il y a un rating, zero sinon\n",
    "\n",
    "for i in range(len(movie)):\n",
    "    t[int(movie[i:(i+1)]['item_id'])][int(movie[i:(i+1)]['user_id'])]=movie[i:(i+1)]['rating']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CollaborativeFiltering(object):\n",
    "    \n",
    "    def __init__(self, dimFactorisation = 5, reg = 0.001, bias = False):\n",
    "        \n",
    "        self.dimFactorisation = dimFactorisation\n",
    "        self.loss_histo = []\n",
    "        self.reg = reg\n",
    "        self.bias = bias\n",
    "        self.loss_histo = []\n",
    "        \n",
    "    def fit(self, users, items, ratings, nb_epochs = 100, learning_rate=0.01, verbose=True):\n",
    "        #Initialisation des deux matrices U et V  (x = u*v)\n",
    "        self.users = users\n",
    "        self.items = items\n",
    "        nbUsers = max(users)\n",
    "        nbItems = max(items)\n",
    "        \n",
    "        \n",
    "        self.u = np.random.rand(self.dimFactorisation, nbUsers)\n",
    "        self.v = np.random.rand(self.dimFactorisation,nbItems)\n",
    "        \n",
    "        #Initialisation eventuelle des biais\n",
    "        if self.bias :\n",
    "            self.usersBias = np.random.rand(nbUsers)\n",
    "            self.itemsBias = np.random.rand(nbItems)\n",
    "            self.globalBias = np.random.rand(1)[0]\n",
    "        \n",
    "        #Nombre de notes \n",
    "        nbRatings = len(ratings)\n",
    "        \n",
    "        for ep in range(nb_epochs):\n",
    "            loss = 0\n",
    "\n",
    "            #Random index\n",
    "            self.ratingIndex = np.random.randint(nbRatings) #Rating tire au hasard\n",
    "\n",
    "            #Utilisateur/item  séléctionnés\n",
    "            self.user_id = int(users[self.ratingIndex]) - 1\n",
    "            self.item_id = int(items[self.ratingIndex]) - 1\n",
    "            self.rating_id = ratings[self.ratingIndex]\n",
    "            \n",
    "            #Estimation actuelle du rating\n",
    "            prediction = self.u[:, self.user_id].dot(self.v[:,self.item_id])\n",
    "            \n",
    "            #Ajout eventuel des biais\n",
    "            if self.bias : \n",
    "                prediction += self.globalBias + self.usersBias[self.user_id] + self.itemsBias[self.item_id]\n",
    "\n",
    "            diff = (self.rating_id - prediction)\n",
    "\n",
    "            loss +=diff**2\n",
    "\n",
    "            factor = 1-self.reg*learning_rate\n",
    "            self.u[:,self.user_id] = factor * self.u[:,self.user_id] + learning_rate * self.v[:,self.item_id] * diff\n",
    "            self.v[:,self.item_id] = factor * self.v[:,self.item_id] + learning_rate * self.u[:,self.user_id] * diff\n",
    "\n",
    "            if self.bias:\n",
    "                self.usersBias[self.user_id] = factor * self.usersBias[self.user_id] + learning_rate * diff\n",
    "                self.itemsBias[self.item_id] = factor * self.itemsBias[self.item_id] + learning_rate * diff\n",
    "                self.globalBias = factor * self.globalBias + learning_rate * diff\n",
    "\n",
    "            self.loss_histo.append(loss)\n",
    "            if verbose :\n",
    "                if(ep%int(nb_epochs/10)==0):\n",
    "                    accuracy_train = np.array([round(self.u[:,(users[i]-1)].dot(self.v[:,(items[i]-1)])) == ratings[i] \n",
    "                                   for i in range(nbRatings)])\n",
    "                    print(\"Accuracy : %d : %f\"%(ep, np.mean(accuracy_train)))\n",
    "\n",
    "        print(\"Apprentissage du modele fini, accuracy final en train : %f\"%(np.mean(accuracy_train)))\n",
    "        \n",
    "        \n",
    "   # round_vecto = np.vectorize(round)\n",
    "    \n",
    "    def predict(self, userIds, itemIds):\n",
    "        preds =[]\n",
    "        for i in range(len(userIds)):   \n",
    "            if itemIds[i] not in self.items :\n",
    "                if userIds[i] not in self.users:\n",
    "                    preds.append(np.random.randint(1,5))\n",
    "                else : \n",
    "                    preds.append(round((np.random.rand(1,self.dimFactorisation).dot(self.v[:,itemIds[i]-1]))[0]))\n",
    "           \n",
    "            elif userIds[i] not in self.users:\n",
    "                preds.append(round(np.random.rand(1,self.dimFactorisation).dot(self.u[:,userIds[i]-1])[0]))\n",
    "\n",
    "            else :\n",
    "                preds.append(round((self.u[:,(userIds[i]-1)].dot(self.v[:,(itemIds[i]-1)]))))\n",
    "        \n",
    "        return np.array(preds)\n",
    "        \n",
    "    def score(self, userIds, itemIds, ratings):\n",
    "        preds = self.predict(userIds, itemIds)\n",
    "        return np.mean(preds.reshape(ratings.shape) == ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0 : 0.074688\n",
      "Accuracy : 5000 : 0.113672\n",
      "Accuracy : 10000 : 0.171875\n",
      "Accuracy : 15000 : 0.229109\n",
      "Accuracy : 20000 : 0.270078\n",
      "Accuracy : 25000 : 0.298000\n",
      "Accuracy : 30000 : 0.316391\n",
      "Accuracy : 35000 : 0.328703\n",
      "Accuracy : 40000 : 0.342469\n",
      "Accuracy : 45000 : 0.350750\n",
      "Apprentissage du modele fini, accuracy final en train : 0.350750\n",
      "Accuracy : 0 : 0.072516\n",
      "Accuracy : 5000 : 0.069875\n",
      "Accuracy : 10000 : 0.064281\n",
      "Accuracy : 15000 : 0.062344\n",
      "Accuracy : 20000 : 0.061281\n",
      "Accuracy : 25000 : 0.060812\n",
      "Accuracy : 30000 : 0.061469\n",
      "Accuracy : 35000 : 0.060078\n",
      "Accuracy : 40000 : 0.059859\n",
      "Accuracy : 45000 : 0.060234\n",
      "Apprentissage du modele fini, accuracy final en train : 0.060234\n",
      "L'accuracy en test est de  0.3461875\n",
      "L'accuracy en test est de  0.05975\n"
     ]
    }
   ],
   "source": [
    "user = np.array(movie['user_id']) \n",
    "item = np.array(movie['item_id']) \n",
    "ratings = np.array(movie['rating'])\n",
    "timestamps = np.array(movie['timestamp'])\n",
    "\n",
    "\n",
    "user_train, user_test, item_train, item_test, ratings_train, ratings_test = train_test_split(\n",
    "            user, item, ratings, train_size=0.8)\n",
    "\n",
    "\n",
    "cf = CollaborativeFiltering(bias=False)\n",
    "cf.fit(user_train, item_train, ratings_train, nb_epochs=50000)\n",
    "\n",
    "print(\"L'accuracy en test est de  \"+\n",
    "      str(1*(cf.predict(user_test,item_test)==ratings_test).mean()))\n",
    "\n",
    "\n",
    "cfBias = CollaborativeFiltering(bias=True)\n",
    "cfBias.fit(user_train, item_train, ratings_train, nb_epochs=50000)\n",
    "\n",
    "print(\"L'accuracy en test est de  \"+\n",
    "      str(1*(cfBias.predict(user_test,item_test)==ratings_test).mean()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loadLinks(path):\n",
    "    d={}\n",
    "    with open(path) as f:\n",
    "        for l in f:\n",
    "            w = l.split()\n",
    "            d[w[0]]= w[1:]\n",
    "    return(d)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "links = loadLinks('/Users/melkigabriel/Desktop/UPMC/FDMS-2/recodata/u.links')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_links = np.zeros((944,944))\n",
    "for i in links.keys():\n",
    "    for k in links[str(i)]:\n",
    "        m_links[int(i),int(k)]=1     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def g(x):\n",
    "    return(4/(1+math.exp(-x)))\n",
    "\n",
    "def g_prime(x):\n",
    "    return(g(x)*(1-g(x)/4))\n",
    "\n",
    "#g = np.vectorize(g)\n",
    "#g_prime = np.vectorize(g_prime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoRec(object):\n",
    "    \n",
    "    def __init__(self, dimFactorisation = 5, bias = False,\n",
    "                    lambda_c=0.1, lambda_u=0.1, lambda_v=0.1, lambda_z=0.1,\n",
    "                    nb_epochs = 1000, learning_rate=0.1):\n",
    "\n",
    "        self.dimFactorisation = dimFactorisation\n",
    "        self.loss_histo = []\n",
    "        self.bias = bias\n",
    "        self.loss_histo = []\n",
    "        \n",
    "        \n",
    "    def fit(self, users, items, ratings, links_data):\n",
    "        \n",
    "        #Initialisation des deux matrices U et V  (x = u*v)\n",
    "        self.users = users\n",
    "        self.items = items\n",
    "        \n",
    "        nbUsers = max(users)\n",
    "        nbItems = max(items)\n",
    "        \n",
    "        #a mettre dans les hyperparametres\n",
    "        # U est de dimensions K x NbUsers\n",
    "        self.u = 5*(np.random.rand(self.dimFactorisation, nbUsers)-0.5)\n",
    "        \n",
    "        # Z est de dimensions K x NbUsers\n",
    "        self.z = 5*(np.random.rand(self.dimFactorisation, nbUsers)-0.5)\n",
    "        \n",
    "        #V est de dimensions NbItems x K\n",
    "        self.v = 5*(np.random.rand(self.dimFactorisation, nbItems)-0.5)\n",
    "        \n",
    "        #Initialisation eventuelle des biais\n",
    "        if self.bias :\n",
    "            self.usersBias = np.random.rand(nbUsers)\n",
    "            self.itemsBias = np.random.rand(nbItems)\n",
    "            self.globalBias = np.random.rand(1)[0]\n",
    "        \n",
    "        #Nombre de notes \n",
    "        nbRatings = len(ratings)\n",
    "        \n",
    "        #SGD\n",
    "        for ep in range(nb_epochs):\n",
    "            loss = 0\n",
    "            \n",
    "            #Random index\n",
    "            self.ratingIndex = np.random.randint(nbRatings) #Rating tire au hasard\n",
    "            self.index_z = np.random.randint(nbUsers)\n",
    "\n",
    "            #Utilisateur/item  séléctionnés\n",
    "            self.user_id = int(users[self.ratingIndex]) - 1\n",
    "            self.item_id = int(items[self.ratingIndex]) - 1\n",
    "            self.rating_id = ratings[self.ratingIndex]\n",
    "            self.link_id = links_data[self.user_id,self.index_z]\n",
    "        \n",
    "            #Estimation actuelle du rating\n",
    "            prediction = g(self.u[:, self.user_id].dot(self.v[:,self.item_id]))\n",
    "            \n",
    "            #Estimation actuelle des links\n",
    "            link_accuracy = g(self.u[:, self.user_id].dot(self.z[:,self.index_z]))\n",
    "            \n",
    "            #Ajout eventuel des biais\n",
    "            if self.bias : \n",
    "                prediction += self.globalBias + self.usersBias[user_id] + self.itemsBias[item_id]\n",
    "\n",
    "            diff_rating = (self.rating_id - prediction)\n",
    "            diff_link = (self.link_id - link_accuracy)\n",
    "\n",
    "            loss += abs(diff_rating)\n",
    "\n",
    "            #Update des facteurs\n",
    "            update_z = learning_rate*(\n",
    "                   lambda_c*g_prime(self.u[:,self.user_id].dot(self.z[:,self.index_z]))*(-diff_link)*self.u[:,self.user_id] \n",
    "                 + lambda_z*self.z[:,self.index_z]\n",
    "            )\n",
    "            \n",
    "            update_u = learning_rate*(\n",
    "                (-diff_rating)*g_prime(self.u[:, self.user_id].dot(self.v[:,self.item_id]))*self.v[:,self.item_id] +\n",
    "                lambda_c*(-diff_link)*g_prime(self.u[:, self.user_id].dot(self.z[:,self.index_z]))*self.z[:,self.index_z] +\n",
    "                lambda_u*self.u[:, self.user_id]\n",
    "            ) \n",
    "            \n",
    "            update_v = learning_rate*(\n",
    "                (-diff_rating)*g_prime(self.u[:, self.user_id].dot(self.v[:,self.item_id]))*self.u[:,self.user_id] +\n",
    "                lambda_v*self.v[:, self.user_id]\n",
    "            ) \n",
    "            \n",
    "            self.z[:,self.index_z] -= update_z\n",
    "            self.u[:,self.user_id] -= update_u\n",
    "            self.v[:,self.item_id] -= update_v\n",
    "\n",
    "            if self.bias:\n",
    "                self.usersBias[user] = factor * self.usersBias[user] + learning_rate * diff\n",
    "                self.itemsBias[item] = factor*self.itemsBias[item] + learning_rate * diff\n",
    "                self.globalBias = factor * self.globalBias + learning_rate * diff\n",
    "\n",
    "            self.loss_histo.append(loss)\n",
    "            \n",
    "            if(ep%int(nb_epochs/10)==0):\n",
    "                accuracy_train = np.array([round(g(self.u[:,(users[i]-1)].dot(self.v[:,(items[i]-1)]))) == ratings[i] \n",
    "                                   for i in range(nbRatings)])\n",
    "                print(\"Accuracy : %d : %f\"%(ep, np.mean(accuracy_train)))\n",
    "            \n",
    "        print(\"Apprentissage du modele fini, accuracy final en train : %f\"%(np.mean(accuracy_train)))\n",
    "        \n",
    "    def predict(self, userIds, itemIds):\n",
    "        preds =[]\n",
    "        for i in range(len(userIds)):   \n",
    "            if itemIds[i] not in self.items :\n",
    "                if userIds[i] not in self.users:\n",
    "                    preds.append(np.random.randint(0,4))\n",
    "                else : \n",
    "                    preds.append(round(g(np.random.rand(1,self.dimFactorisation).dot(self.v[:,itemIds[i]-1])[0])))\n",
    "           \n",
    "            elif userIds[i] not in self.users:\n",
    "                preds.append(round(g(np.random.rand(1,self.dimFactorisation).dot(self.u[:,userIds[i]-1])[0])))\n",
    "\n",
    "            else :\n",
    "                preds.append(round(g(self.u[:,(userIds[i]-1)].dot(self.v[:,(itemIds[i]-1)]))))\n",
    "        \n",
    "        return np.array(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0 : 0.164453\n",
      "Accuracy : 240000 : 0.274391\n",
      "Accuracy : 480000 : 0.303687\n",
      "Accuracy : 720000 : 0.338953\n",
      "Accuracy : 960000 : 0.360047\n",
      "Accuracy : 1200000 : 0.373797\n",
      "Accuracy : 1440000 : 0.378891\n",
      "Accuracy : 1680000 : 0.381359\n",
      "Accuracy : 1920000 : 0.384906\n",
      "Accuracy : 2160000 : 0.384344\n",
      "Apprentissage du modele fini, accuracy final en train : 0.384344\n",
      "L'accuracy en test est de  0.329875\n",
      "La loss en test est de  0.9235625\n"
     ]
    }
   ],
   "source": [
    "user = np.array(movie['user_id']) \n",
    "item = np.array(movie['item_id']) \n",
    "ratings = np.array(movie['rating']) - 1\n",
    "timestamps = np.array(movie['timestamp'])\n",
    "\n",
    "\n",
    "user_train, user_test, item_train, item_test, ratings_train, ratings_test = train_test_split(user, item, ratings, train_size=0.8)\n",
    "lambdada=0.6\n",
    "r = 7\n",
    "\n",
    "cf = SoRec(dimFactorisation=r)\n",
    "cf.fit(user_train, item_train, ratings_train, m_links, nb_epochs=400*100*60,\n",
    "       lambda_c=lambdada, lambda_u=lambdada, lambda_v=lambdada, lambda_z=lambdada,\n",
    "       learning_rate=0.01\n",
    "          )\n",
    "\n",
    "#testons\n",
    "print(\"L'accuracy en test est de  \" +\n",
    "      str(1*(cf.predict(user_test,item_test) == ratings_test).mean()))\n",
    "\n",
    "print(\"La loss en test est de  \" +\n",
    "      str(np.abs(cf.predict(user_test,item_test) - ratings_test).mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
